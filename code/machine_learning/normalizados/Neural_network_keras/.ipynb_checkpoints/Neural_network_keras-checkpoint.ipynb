{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_default_dlopen_flags\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcusolver.so.8.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c7951f8dee7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "import joypy #el de las densidades guays\n",
    "\n",
    "from sklearn import manifold  \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os \n",
    "#cwd = os.getcwd()\n",
    "cwd=\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/\"\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from keras.models import Sequential \n",
    "#from keras.layers import  Dense, Flatten, Activation, Dropout\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras import optimizers\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings('ignore') #ATENCION QUE ESTO CREO QUE FUNCIONA PARA TODO EL NOTEBOOK\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL THE DATASET\n",
    "\n",
    "df=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/trainpickle\")\n",
    "df_originaltest=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/testpickle\")\n",
    "\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will follow the scikit-learn schema to train and validate the model**\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores  \n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def grid_model(optimizer='adam', activation='relu', neurons =5, hidden_layers=1,dropout=0.0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #first layer\n",
    "    model.add(Dense(neurons,activation=activation, input_shape=(16,)))\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        # Add one hidden layer\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        \n",
    "    #last layer: \n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    optimizer= optimizers.Adam() #by default lr=0.001\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model  #Jordi dijo categorical cross entropy pero me da error. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Scores(y_true,y_pred):\n",
    "    \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    prec=tp / (tp + fp)\n",
    "    recall= tp / (tp + fn)\n",
    "    F1_score= 2 * (prec * recall) / (prec + recall)\n",
    "    acc=metrics.accuracy_score(y_true,y_pred)\n",
    "    kappa_cohen=metrics.cohen_kappa_score(y_true,y_pred)\n",
    "        \n",
    "    return(tn, fp, fn, tp, acc, prec,recall, F1_score,kappa_cohen)\n",
    "\n",
    "def model(X_train,y_train,X_test):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler=scaler.fit(X_train)    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    model = KerasClassifier(build_fn=grid_model, verbose=0)\n",
    "\n",
    "        \n",
    "    #batch_size = [10, 20, 50, 100]\n",
    "    #epochs = [50, 100,150]\n",
    "    #neurons= [10,50,100,150,200,300]\n",
    "    #hidden_layers=[3,4,5,6]\n",
    "    #dropout= [0,0.1,0.2,0.3]\n",
    "    \n",
    "    batch_size = [10, 25]\n",
    "    epochs = [10,50]\n",
    "    neurons= [10,100,150]\n",
    "    hidden_layers=[3,4,5]\n",
    "    dropout= [0,0.2]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    param_grid = dict(batch_size=batch_size, epochs=epochs, neurons=neurons, hidden_layers= hidden_layers,dropout=dropout)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid,  cv=5) #n_jobs=-1, solo 1 cpus\n",
    "\n",
    "    # define the grid search parameters\n",
    "    #optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    \n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "            \n",
    "    y_pred_proba = best_model.model.predict_proba(X_test) #Neccesary to make the ROC curve \n",
    "    y_pred =  best_model.predict(X_test)\n",
    "\n",
    "    return( grid.best_params_,best_model,y_pred,y_pred_proba)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric df: \n",
    "df_metrics =  pd.DataFrame(index=[750,500], columns=[\"tn\", \"fp\", \"fn\", \"tp\", \"acc\", \"prec\",\"recall\",\"F1_score\",\"kappa_cohen\",\"auc\"])\n",
    "\n",
    "File_object =  open(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/a.txt\",\"a\") \n",
    "File_object.write(\"dddd\")\n",
    "File_object.close()\n",
    "\n",
    "for i in (750,500):\n",
    "    \n",
    "    File_object =  open(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/a.txt\",\"a\") \n",
    "    File_object.write(str(i))\n",
    "    File_object.close()\n",
    "\n",
    "    print('mass=', i )\n",
    "\n",
    "    #train\n",
    "      #0 same size of dfmass1000 #TENER EN CUENTA QUE AQUI QUE LA DE 0 SEA MAYOR SIZE ES CASUALIDAD\n",
    "    #EN OTRO PICKE PODRIA SER AL REVES, ESTO SE DEBERIA DE ARREGLAR\n",
    "    np.random.seed(1)\n",
    "    #dfmass0['mass'] = np.random.choice([500,750,1000,1250,1500], dfmass0.shape[0])#las cambio para queno sean 0 porque entonces sabría el label\n",
    "    dfmass1000=df.loc[df['mass'] ==i]        \n",
    "    dfmass0=df.loc[df['mass'] == 0].sample(random_state=1,n=dfmass1000.shape[0])   #cojo todos!!!!!!!!!!!!!! \n",
    "    dfmass1000=pd.concat([dfmass1000, dfmass0]).sample(random_state=1,frac=1).reset_index(drop=True)    #concatenating and shuffling\n",
    "    dfmass1000['mass']=dfmass1000['mass'].astype(float)\n",
    "    dfmass1000=dfmass1000.sample(random_state=1,n=20000*5)\n",
    "    print(\"train\", pd.crosstab(dfmass1000['label'],dfmass1000['mass']))\n",
    "    dfmass1000=dfmass1000.drop('mass', axis=1)        #la masa es una caracterísitca\n",
    "    \n",
    "    \n",
    "    #test\n",
    "    dfmass1000test=df_originaltest.loc[df_originaltest['mass'] == i]\n",
    "    dfmass0test=df_originaltest.loc[df_originaltest['mass'] == 0].sample(random_state=1,n=dfmass1000test.shape[0])\n",
    "    dfmass1000test=pd.concat([dfmass1000test, dfmass0test]).sample(frac=1).reset_index(drop=True)\n",
    "    dfmass1000test=dfmass1000test.sample(random_state=1,n=10000*5)\n",
    "\n",
    "    print(\"test\", pd.crosstab(dfmass1000test['label'],dfmass1000test['mass']))  \n",
    "    \n",
    "    dfmass1000test=dfmass1000test.drop('mass', axis=1)  \n",
    "    \n",
    "    \n",
    "\n",
    "    X_train=dfmass1000.drop(['label'], axis=1)\n",
    "    X_train=X_train.drop(['lep_phi','met_phi','jets_no','jet1_phi','jet1_btag','jet2_phi','jet2_btag','jet3_phi','jet3_btag','jet4_phi', 'jet4_btag'], axis=1)\n",
    "    y_train=dfmass1000.label\n",
    "    \n",
    "    X_test=dfmass1000test.drop(['label'], axis=1)\n",
    "    X_test=X_test.drop(['lep_phi','met_phi','jets_no','jet1_phi','jet1_btag','jet2_phi','jet2_btag','jet3_phi','jet3_btag','jet4_phi', 'jet4_btag'], axis=1)\n",
    "    y_test=dfmass1000test.label\n",
    "        \n",
    "   # print(X_train)\n",
    "   # print(y_test)\n",
    "    \n",
    "    #try:\n",
    "\n",
    "    grid,best_model,y_pred,y_pred_proba = model(X_train,y_train,X_test)\n",
    "    print(best_model)\n",
    "    \n",
    "    file2write=open(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/df\"+ str(i)+ \".txt\",'w')\n",
    "    file2write.write(str(grid))\n",
    "    file2write.close()\n",
    "\n",
    "\n",
    "#Roc curve construction\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    #plt.savefig(cwd2+'MASS=' + str(i) + 'ROC.png')\n",
    "    plt.show()\n",
    "    plt.clf() \n",
    "\n",
    "#METRICS \n",
    "\n",
    "    tn, fp, fn, tp, acc, prec,recall, F1_score,kappa_cohen=Scores(y_test,y_pred)\n",
    "\n",
    "    df_metrics.tn[i]=tn\n",
    "    df_metrics.fp[i]=fp\n",
    "    df_metrics.fn[i]=fn\n",
    "    df_metrics.tp[i]=tp\n",
    "    df_metrics.acc[i]=acc\n",
    "    df_metrics.prec[i]=prec\n",
    "    df_metrics.recall[i]=recall\n",
    "    df_metrics.F1_score[i]=F1_score\n",
    "    df_metrics.kappa_cohen[i]=kappa_cohen\n",
    "    df_metrics.auc[i]=auc\n",
    "        \n",
    "    #except:\n",
    "       # print(\"No funcionó :(\")\n",
    "    df_metrics.to_pickle(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/df\"+ str(i)+ \".pkl\") \n",
    "\n",
    "\n",
    "print(\"X_train_shape\",X_train.shape)\n",
    "print(\"X_test_shape\",X_test.shape)\n",
    "df_metrics.to_pickle(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/df_metrics.pkl\") \n",
    "\n",
    "\n",
    "#display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"acc\"], label=\"accuracy\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"F1_score\"], label=\"F1_score\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"kappa_cohen\"], label=\"kappa\")\n",
    "\n",
    "ax.set_xlabel('mass')\n",
    "ax.set_ylabel('Metric')\n",
    "legend = ax.legend(fontsize='x-large')\n",
    "plt.show()\n",
    "plt.savefig(cwd+ 'metrics_comparation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_pickle(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/df_metrics.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/home/angela/Notebook/machine_learning/normalizados/Neural_network_keras/df500.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>kappa_cohen</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>13157</td>\n",
       "      <td>11792</td>\n",
       "      <td>4818</td>\n",
       "      <td>20233</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>0.631788</td>\n",
       "      <td>0.807672</td>\n",
       "      <td>0.708985</td>\n",
       "      <td>0.335219</td>\n",
       "      <td>0.729616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tn     fp    fn     tp     acc      prec    recall  F1_score  \\\n",
       "500  13157  11792  4818  20233  0.6678  0.631788  0.807672  0.708985   \n",
       "750    NaN    NaN   NaN    NaN     NaN       NaN       NaN       NaN   \n",
       "\n",
       "    kappa_cohen       auc  \n",
       "500    0.335219  0.729616  \n",
       "750         NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
