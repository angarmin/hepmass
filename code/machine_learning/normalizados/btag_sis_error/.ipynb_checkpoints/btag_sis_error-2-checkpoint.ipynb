{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "import joypy #el de las densidades guays\n",
    "\n",
    "from sklearn import manifold  \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os \n",
    "cwd = os.getcwd() + \"/\"\n",
    "cwd =\"/home/angela/Notebook/machine_learning/normalizados/btag_sis_error/\"\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') #ATENCION QUE ESTO CREO QUE FUNCIONA PARA TODO EL NOTEBOOK\n",
    "\n",
    "import random\n",
    "random.seed(6)\n",
    "np.random.seed(6)\n",
    "np.random.RandomState(6)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si suponemos que el 65 % es lo que conseguimos sin forzar mucho el método ( me refiero en el análisis de física en ATLAS) , podemos fijarlo en 65%\n",
    "ese 35% puedes ponderarlo con las proporciones que has obtenido: así tendremos 13*35/13.6 para 1 b tag y 0.6*35/13.6 para 0 tag , ¿lo ves?\n",
    "Porque en datos toy simulados reconstruimos sin ambigüedades, tenemos los 4 jets más energéticos (es un caso casi ideal) , datos simulado detallados y en dato reales hay más mala concordancia entre lo que se reconstruye e identifica y lo que es 'truth'\n",
    "En el proceso de reconsturcción de jets y de elegir jets que sean los del esquema de decay de los tops nos estamos equivocando ( mismatch).\n",
    "ampliando lo fake hasta el 35% estás siendo más realista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL THE DATASET\n",
    "\n",
    "df=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/trainpickle\")\n",
    "df_originaltest=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/testpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMPLe DATASET\n",
    "#df=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/trainsimplepickle\").sample(n=1000, random_state=1)\n",
    "#df_originaltest=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/testsimplepickle\").sample(n=300, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    86.471414\n",
       "1.0    12.954186\n",
       "0.0     0.574400\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1).value_counts()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will follow the scikit-learn schema to train and validate the model**\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores\n",
    "\n",
    "\n",
    "def Scores(y_true,y_pred):\n",
    "    \n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    prec=tp / (tp + fp)\n",
    "    recall= tp / (tp + fn)\n",
    "    F1_score= 2 * (prec * recall) / (prec + recall)\n",
    "    acc=metrics.accuracy_score(y_true,y_pred)\n",
    "    kappa_cohen=metrics.cohen_kappa_score(y_true,y_pred)\n",
    "        \n",
    "    return(tn, fp, fn, tp, acc, prec,recall, F1_score,kappa_cohen)\n",
    "\n",
    "\n",
    "def model(X_train,y_train,X_test):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler=scaler.fit(X_train)\n",
    "    \n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    model = MLPClassifier() \n",
    "    \n",
    "    #grid search, copy of kaggle: https://www.kaggle.com/hatone/mlpclassifier-with-gridsearchcv#L72\n",
    "    #defalult (scikit learn) :  best_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2)))\n",
    "\n",
    "    #params = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}    #model = SVC(probability=True) \n",
    "    #reduced: \n",
    "    params = {'solver': ['lbfgs'], 'max_iter': [200,600,1000,1400,1800], 'alpha': 10.0 ** -np.arange(4, 6), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[6]}    #model = SVC(probability=True) \n",
    "\n",
    "    grid = GridSearchCV(estimator=model, param_grid=params,cv=5,verbose=1, n_jobs=-1)\n",
    "\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    best_model.fit(X_train,y_train)\n",
    "    # Predict test set labels\n",
    "    y_pred = best_model.predict(X_test)    \n",
    "    y_pred_proba = best_model.predict_proba(X_test)[::,1] #Neccesary to make the ROC curve \n",
    "\n",
    "    return(grid,best_model,y_pred,y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test bien y variando el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd2=cwd+\"train2b\"\n",
    "\n",
    "#cargo otra vez para resetear el df\n",
    "df=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/trainpickle\")\n",
    "df_originaltest=pd.read_pickle(\"/home/angela/Notebook/data/normalizados/testpickle\")\n",
    "df_originaltest=df_originaltest[df_originaltest.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==2]\n",
    "\n",
    "print(\"test bien y variando el train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"B_tag_train_prop\", \"\\n\" ,df_originaltest.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1).value_counts()/df_originaltest.shape[0]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\", pd.crosstab(df_originaltest['label'],df_originaltest['mass']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric df: \n",
    "\n",
    "\n",
    "#CONSTRUCCIÓN DATAFRAME\n",
    "\n",
    "    \n",
    "for btag in [\"all0\",\"all1\",\"all2\",\"orig\",\"real\",]:\n",
    "    \n",
    "    df_metrics = pd.DataFrame(index=[500,750,1000,1250,1500], columns=[\"tn\", \"fp\", \"fn\", \"tp\", \"acc\", \"prec\",\"recall\",\n",
    "                                                                   \"F1_score\",\"kappa_cohen\",\"auc\"])\n",
    "    print(\"BTAG =\", btag)\n",
    "    \n",
    "    df_btag=df.copy()\n",
    "    \n",
    "    if btag==\"all0\":\n",
    "        df_btag=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==0]\n",
    "    elif btag==\"all1\":\n",
    "        df_btag=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==1]\n",
    "        df_btag=df_btag.sample(df_btag.shape[0], random_state=6).reset_index(drop=True)\n",
    "    elif btag==\"all2\":                          \n",
    "        df_btag=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==2]\n",
    "        df_btag=df_btag.sample(df_btag.shape[0], random_state=6).reset_index(drop=True)\n",
    "\n",
    "    elif btag==\"orig\":                          \n",
    "        df_btag=df_btag.sample(df_btag.shape[0], random_state=6).reset_index(drop=True)\n",
    "\n",
    "    elif btag==\"real\": #65,33,2\n",
    "        df0=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==0]\n",
    "        df1=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==1].sample(n=int(df0.shape[0]*(33/2)), random_state=6, replace=False)\n",
    "        df2=df_btag[df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1)==2].sample(n=int(df0.shape[0]*(65/2)), random_state=6, replace=False)\n",
    "        frames = [df1, df2, df0]\n",
    "        df_btag = pd.concat(frames)\n",
    "        df_btag=df_btag.sample(df_btag.shape[0], random_state=6).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    for i in (500,750,1000,1250,1500):\n",
    "\n",
    "        print('mass=', i )\n",
    "\n",
    "        dfmass1000=df_btag.loc[df_btag['mass'] == i]\n",
    "        dfmass0=df_btag.loc[df_btag['mass'] == 0].sample(n=dfmass1000.shape[0], random_state=6, replace=False)        #0 same size of dfmass1000\n",
    "        dfmass1000=pd.concat([dfmass1000, dfmass0]).sample(frac=1).reset_index(drop=True)    #concatenating and shuffling\n",
    "        dfmass1000=dfmass1000.drop('mass', axis=1) \n",
    "\n",
    "        #test\n",
    "\n",
    "        \n",
    "        #test\n",
    "        dfmass1000test=df_originaltest.loc[df_originaltest['mass'] == i]\n",
    "        dfmass0test=df_originaltest.loc[df_originaltest['mass'] == 0].sample(random_state=6,n=dfmass1000test.shape[0])\n",
    "        np.random.seed(6)\n",
    "        dfmass1000test=pd.concat([dfmass1000test, dfmass0test]).sample(frac=1).reset_index(drop=True)\n",
    "        dfmass1000test=dfmass1000test.drop('mass', axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "        dfmass1000=dfmass1000.sample(n=9300, random_state=6)\n",
    "        dfmass1000test=dfmass1000test.sample(n=4650, random_state=6)\n",
    "        \n",
    "        print(\"B_tag_train_prop\", \"\\n\" ,df_btag.loc[:,['jet1_btag','jet2_btag',\"jet3_btag\",\"jet4_btag\"]].sum(axis=1).value_counts()/df_btag.shape[0]*100)\n",
    "\n",
    "\n",
    "        X_train=dfmass1000.drop(['label'], axis=1)\n",
    "        y_train=dfmass1000.label\n",
    "\n",
    "        X_test=dfmass1000test.drop(['label'], axis=1)\n",
    "        y_test=dfmass1000test.label\n",
    "\n",
    "        print(\"X_train_shape\",X_train.shape)\n",
    "        print(\"X_test_shape\",X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        grid,best_model,y_pred,y_pred_proba = model(X_train,y_train,X_test)\n",
    "        print(best_model)\n",
    "\n",
    "\n",
    "    #Roc curve construction\n",
    "\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "        plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "        plt.legend(loc=4)\n",
    "        #plt.savefig(cwd2+'MASS=' + str(i) + 'ROC.png')\n",
    "        plt.show()\n",
    "        plt.clf() \n",
    "\n",
    "    #METRICS \n",
    "\n",
    "        tn, fp, fn, tp, acc, prec,recall, F1_score,kappa_cohen=Scores(y_test,y_pred)\n",
    "\n",
    "        df_metrics.tn[i]=tn\n",
    "        df_metrics.fp[i]=fp\n",
    "        df_metrics.fn[i]=fn\n",
    "        df_metrics.tp[i]=tp\n",
    "        df_metrics.acc[i]=acc\n",
    "        df_metrics.prec[i]=prec\n",
    "        df_metrics.recall[i]=recall\n",
    "        df_metrics.F1_score[i]=F1_score\n",
    "        df_metrics.kappa_cohen[i]=kappa_cohen\n",
    "        df_metrics.auc[i]=auc\n",
    "\n",
    "\n",
    "    display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "y_test.value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''fig, ax = plt.subplots()\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"accuracy\"], label=\"accuracy\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"sensitibity\"], label=\"sensitibity\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"specifity\"], label=\"specifity\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"auc\"], label=\"auc\")\n",
    "ax.plot([\"500\",\"750\",\"1000\",\"1250\",\"1500\"], df_metrics[\"kappa\"], label=\"kappa\")\n",
    "\n",
    "ax.set_xlabel('mass')\n",
    "ax.set_ylabel('Metric')\n",
    "legend = ax.legend(fontsize='x-large')\n",
    "plt.show()\n",
    "#plt.savefig(cwd+ 'metrics_comparation.png')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
